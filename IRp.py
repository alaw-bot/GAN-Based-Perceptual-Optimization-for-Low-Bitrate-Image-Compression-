# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x7H8ZZwslUYvY7mDmR8JmNjQq6j6rC16
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import vgg19, mobilenet_v2
from torchvision import transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# Models

class Encoder(nn.Module):
    def __init__(self, in_channels=3, latent_dim=256):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(256, latent_dim, 4, 2, 1), nn.ReLU()
        )

    def forward(self, x):
        return self.encoder(x)

class Generator(nn.Module):
    def __init__(self, out_channels=3, latent_dim=256):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, 256, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, out_channels, 4, 2, 1), nn.Sigmoid()
        )

    def forward(self, z):
        return self.decoder(z)

class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1), nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, 2, 1), nn.LeakyReLU(0.2),
            nn.Conv2d(256, 1, 4, 1, 1)
        )

    def forward(self, x):
        return self.model(x)
#saliency mapping
class SaliencyMap(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = mobilenet_v2(pretrained=True).features.to(device)
        for p in self.model.parameters():
            p.requires_grad = False

    def forward(self, x):
        with torch.no_grad():
            saliency = self.model(x)
            saliency = F.adaptive_avg_pool2d(saliency, x.shape[-2:])
            saliency = saliency.mean(1, keepdim=True)
            return saliency / (saliency.max() + 1e-8)

class PerceptualLoss(nn.Module):
    def __init__(self):
        super().__init__()
        vgg = vgg19(pretrained=True).features[:16]
        self.vgg = nn.Sequential(*vgg).to(device)
        for p in self.vgg.parameters():
            p.requires_grad = False

    def forward(self, real, fake):
        real_feats = self.vgg(real)
        fake_feats = self.vgg(fake)
        return F.mse_loss(real_feats, fake_feats)


transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

train_data = CIFAR10(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(train_data, batch_size=16, shuffle=True)


#Initialize

encoder = Encoder().to(device)
generator = Generator().to(device)
discriminator = Discriminator().to(device)
saliency_net = SaliencyMap()
perceptual_loss_fn = PerceptualLoss()

opt_G = torch.optim.Adam(list(encoder.parameters()) + list(generator.parameters()), lr=1e-4)
opt_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4)

#Train

EPOCHS = 25
latent_save_dir = "compressed_latents"
os.makedirs(latent_save_dir, exist_ok=True)

for epoch in range(EPOCHS):
    encoder.train(); generator.train(); discriminator.train()
    total_g, total_d = 0, 0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")

    for i, (imgs, _) in enumerate(pbar):
        real_img = imgs.to(device)
        z = encoder(real_img)
        recon = generator(z)
        saliency = saliency_net(real_img)

        # Losses
        loss_saliency = torch.mean(torch.abs(recon - real_img) * saliency)
        loss_perceptual = perceptual_loss_fn(real_img, recon)
        pred_fake = discriminator(recon)
        loss_gan = F.binary_cross_entropy_with_logits(pred_fake, torch.ones_like(pred_fake))
        loss_G = loss_gan + 0.5 * loss_perceptual + 0.3 * loss_saliency

        opt_G.zero_grad()
        loss_G.backward()
        opt_G.step()

        pred_real = discriminator(real_img)
        pred_fake = discriminator(recon.detach())
        loss_D_real = F.binary_cross_entropy_with_logits(pred_real, torch.ones_like(pred_real))
        loss_D_fake = F.binary_cross_entropy_with_logits(pred_fake, torch.zeros_like(pred_fake))
        loss_D = 0.5 * (loss_D_real + loss_D_fake)

        opt_D.zero_grad()
        loss_D.backward()
        opt_D.step()

        total_g += loss_G.item()
        total_d += loss_D.item()
        pbar.set_postfix(loss_G=loss_G.item(), loss_D=loss_D.item())

        # Save latent to binary
        z_np = z.detach().cpu().numpy()
        for b in range(z_np.shape[0]):
            with open(os.path.join(latent_save_dir, f"img_{epoch}_{i}_{b}.bin"), "wb") as f:
                f.write(z_np[b].tobytes())

    print(f"[Epoch {epoch+1}] Avg Generator Loss: {total_g/len(train_loader):.4f}, Discriminator Loss: {total_d/len(train_loader):.4f}")

    #sample reconstruction
    with torch.no_grad():
        z = encoder(real_img)
        recon = generator(z)
        fig, axs = plt.subplots(1, 2, figsize=(6,3))
        axs[0].imshow(real_img[0].cpu().permute(1, 2, 0))
        axs[0].set_title("Original")
        axs[1].imshow(recon[0].cpu().permute(1, 2, 0))
        axs[1].set_title("Reconstructed")
        for ax in axs:
            ax.axis('off')
        plt.tight_layout()
        plt.show()